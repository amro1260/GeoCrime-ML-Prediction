# -*- coding: utf-8 -*-
"""Geospatial-analysis & ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U3ZUpOP5jumy8mOLkISPk1jhR2zyyeJk

# Geospatial Analysis and Modelling of Health Assets and Crimes
"""

import pandas as pd
import numpy as np
import geopandas as gpd
import re
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import matplotlib.colors as mcolors
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from scipy.stats import gaussian_kde

# Load the updated dataset for geospatial analysis
health_crime_df = pd.read_csv('AHAH_Crimes.csv')

# Display the first few rows to confirm the structure and the specific columns of interest
health_crime_df[['lsoa11', 'Latitude', 'Longitude', 'Average Crimes', 'ah3h', 'ah3g', 'ah3e', 'ah3r', 'ah3ahah']].head()

health_crime_df[['ah3h', 'ah3g', 'ah3e', 'ah3r', 'ah3ahah', 'Average Crimes']].describe()

# Convert the DataFrame to a GeoDataFrame
gdf = gpd.GeoDataFrame(health_crime_df, geometry=gpd.points_from_xy(health_crime_df.Longitude, health_crime_df.Latitude))

# Define a function for creating maps
def create_map(gdf, column, title):
    fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="5%", pad=0.1)
    gdf.plot(column=column, ax=ax, legend=True, cax=cax, cmap='RdYlGn_r')
    ax.set_title(title)
    plt.show()

# Create maps for each variable
create_map(gdf, 'Average Crimes', 'Spatial Distribution of Average Crimes')
create_map(gdf, 'ah3h', 'Health Domain Score')
create_map(gdf, 'ah3g', 'Green/Bluespace Domain Score')
create_map(gdf, 'ah3e', 'Air Quality Domain Score')
create_map(gdf, 'ah3r', 'Retail Domain Score')
create_map(gdf, 'ah3ahah', 'Access to Healthy Assets and Hazards Index Score')

"""To visually examine the spatial relationships between health assets, hazards, and crime rates, we will focus on the following:

- Access to Healthy Assets and Hazards Index Score (as a measure of health assets and hazards).
- Average Crimes (as a measure of crime rates).

We will create a map that overlays these two aspects. This visualization will allow us to observe areas where high crime rates coincide with poor access to healthy assets and vice versa.
"""

# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(15, 8))

# Plotting the average crimes on the first subplot
gdf.plot(column='Average Crimes', ax=axes[0], legend=True, cmap='Reds', alpha=0.7, legend_kwds={'label': "Average Crimes"})
axes[0].set_title('Spatial Distribution of Average Crimes')
axes[0].set_xlabel('Longitude')
axes[0].set_ylabel('Latitude')

# Plotting the Access to Healthy Assets and Hazards Index Score on the second subplot
gdf.plot(column='ah3ahah', ax=axes[1], legend=True, cmap='Greens', alpha=0.7, legend_kwds={'label': "Access to Healthy Assets and Hazards Index Score"})
axes[1].set_title('Access to Healthy Assets and Hazards Index Score')
axes[1].set_xlabel('Longitude')
axes[1].set_ylabel('Latitude')

plt.tight_layout()
plt.show()

"""The map above visualizes the spatial relationships between health assets, hazards (as indicated by the Access to Healthy Assets and Hazards Index Score), and crime rates (as indicated by Average Crimes).

In this overlay:

- The green areas represent better access to healthy assets and fewer hazards.
The red areas indicate higher crime rates.
- Areas where green and red overlap may indicate regions with high crime rates but also good access to healthy assets, or vice versa.

This visualization helps in identifying patterns and potential areas of concern or interest for further analysis or policy-making.

### Modeling using ML
"""

# Selecting the features and the target variable
features = ['ah3h', 'ah3g', 'ah3e', 'ah3r', 'ah3ahah']  # Health assets and hazards indicators
target = 'Average Crimes'  # Crime rates

# Preparing the feature matrix (X) and target vector (y)
X = health_crime_df[features]
y = health_crime_df[target]

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initializing the linear regression model
linear_model = LinearRegression()

# Training the model
linear_model.fit(X_train, y_train)

# Making predictions on the test set
y_pred = linear_model.predict(X_test)

# Evaluating the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

mse, r2

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor

# Initialize the models
random_forest_model = RandomForestRegressor(random_state=42)
gbm_model = GradientBoostingRegressor(random_state=42)
neural_network_model = MLPRegressor(random_state=42, max_iter=500)

# Function to train and evaluate a model
def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):
    # Train the model
    model.fit(X_train, y_train)
    # Make predictions
    y_pred = model.predict(X_test)
    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    return mse, r2

# Training and evaluating Random Forest model
rf_mse, rf_r2 = train_and_evaluate_model(random_forest_model, X_train, y_train, X_test, y_test)

# Training and evaluating GBM model
gbm_mse, gbm_r2 = train_and_evaluate_model(gbm_model, X_train, y_train, X_test, y_test)

# Training and evaluating Neural Network model
nn_mse, nn_r2 = train_and_evaluate_model(neural_network_model, X_train, y_train, X_test, y_test)

rf_mse, rf_r2, gbm_mse, gbm_r2, nn_mse, nn_r2

"""Here are the results from training and evaluating the various models:

**Random Forest**
- Mean Squared Error (MSE): Approximately 3814.67
- R-squared (R²): Approximately 0.43

**Gradient Boosting Machine (GBM)**
- Mean Squared Error (MSE): Approximately 4382.32
- R-squared (R²): Approximately 0.34

**Neural Network**
- Mean Squared Error (MSE): Approximately 5341.78
- R-squared (R²): Approximately 0.20

From these results, we observe the following:

The Random Forest model performed the best in terms of both MSE and R², indicating it is the most effective at capturing the relationship between health assets/hazards indicators and crime rates in this dataset.

The GBM model also shows a reasonable performance but is slightly less effective than the Random Forest model.

The Neural Network has the lowest R² and highest MSE, suggesting it may not be as well-suited to this specific task or may require more fine-tuning and optimization.
"""

import folium

# Creating a base map centered around the mean latitude and longitude of the dataset
base_map = folium.Map(location=[health_crime_df['Latitude'].mean(), health_crime_df['Longitude'].mean()], zoom_start=10, tiles='cartodbpositron')

# Defining a function to create layers for each health variable
def add_layer(variable, name, colormap):
    # Creating a feature group for this specific variable
    feature_group = folium.FeatureGroup(name=name)
    for _, row in health_crime_df.iterrows():
        # Create a circle marker for each point
        folium.CircleMarker(
            location=[row['Latitude'], row['Longitude']],
            radius=3,  # Fixed radius for all points
            color=colormap(row[variable]),  # Color depends on the variable value
            fill=True,
            fill_opacity=0.7
        ).add_to(feature_group)
    feature_group.add_to(base_map)

# Adding layers to the map for each specified variable
add_layer('ah3h', 'Health Domain Score', colormap=folium.LinearColormap(['white', 'blue']))
add_layer('ah3g', 'Green/Bluespace Domain Score', colormap=folium.LinearColormap(['white', 'green']))
add_layer('ah3e', 'Air Quality Domain Score', colormap=folium.LinearColormap(['white', '#808080']))
add_layer('ah3r', 'Retail Domain Score', colormap=folium.LinearColormap(['white', 'yellow']))
add_layer('ah3ahah', 'Access to Healthy Assets & Hazards Index', colormap=folium.LinearColormap(['white', 'red']))

# Adding a layer control to switch between layers
folium.LayerControl().add_to(base_map)

# Displaying the map
base_map.save('health_assets_map.html')

# Extracting the coordinates and crime rates
x = gdf['Longitude']
y = gdf['Latitude']
crime_rates = gdf['Average Crimes']

# Calculate the point density
xy = np.vstack([x,y])
z = gaussian_kde(xy)(xy)

# Sort the points by density so that the densest points are plotted last
idx = z.argsort()
x, y, z = x[idx], y[idx], z[idx]

fig, ax = plt.subplots(figsize=(11, 9))
sc = ax.scatter(x, y, c=z, s=50, edgecolor=None, cmap='RdYlGn_r')
plt.colorbar(sc, ax=ax, label='Density of Average Crimes')

plt.title('Heat Map of Average Crimes (2020-23)')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.grid(True)
plt.show()

"""## Finding Top 3 LSOAs

To find the LSOA with the best trade-off between crimes and health variables and is most suited for living, we need to define a scoring system that reflects both the crime rate and health index's importance. We'll assign weights to these variables and create a composite score for each LSOA. Then, we'll perform optimization to find the LSOA with the best (highest or lowest, depending on the perspective) composite score.

- Both crime rates and health index scores are normalized to have a comparable scale.
- Determine weights for the average crimes and health index (ah3ahah). A typical approach might be to assign higher weight to health index if we prioritize living conditions and lower weight to crime if we are more tolerant of it, or vice versa.
- Calculate a composite score for each LSOA.
- Find the LSOA with the best composite score.

We have used a simple linear combination:
Composite Score=w1×Normalized Health Index−w2×Normalized Crime Rate

Here, w1 and w2 are the weights for the health index and crime rate, respectively. Note that we are subtracting the normalized crime rate to indicate that a higher crime rate is less desirable.

After defining these, we will find the LSOA with the highest composite score.

Let's define the weights as w1=0.7 (for health index) and w2=0.3 (for crime rate), assuming a higher priority is given to health conditions.
"""

# Normalize the crime rates
crime_min, crime_max = health_crime_df['Average Crimes'].min(), health_crime_df['Average Crimes'].max()

# Normalize each of the health variables
for col in ['ah3h', 'ah3g', 'ah3e', 'ah3r', 'ah3ahah']:
    min_val = health_crime_df[col].min()
    max_val = health_crime_df[col].max()
    health_crime_df[f'Normalized {col}'] = (health_crime_df[col] - min_val) / (max_val - min_val)

# Assign equal weights to each health variable and calculate the aggregated health score
num_health_vars = 5  # Total number of health variables
weights_health = [1/num_health_vars] * num_health_vars  # Equal weights for each health variable

# Assign weights
w_health = 0.7  # Weight for the health index
w_crime = 0.3   # Weight for the crime rate

health_crime_df['Normalized Crime Rate'] = (health_crime_df['Average Crimes'] - crime_min) / (crime_max - crime_min)
health_crime_df['Aggregated Health Score'] = health_crime_df[['Normalized ah3h', 'Normalized ah3g', 'Normalized ah3e', 'Normalized ah3r', 'Normalized ah3ahah']].dot(weights_health)

# Calculate the composite score with the aggregated health score
health_crime_df['Composite Score'] = (w_health * health_crime_df['Aggregated Health Score']) - (w_crime * health_crime_df['Normalized Crime Rate'])

# Find the top three LSOAs with the highest new composite score
top_lsoas = health_crime_df.nlargest(3, 'Composite Score')[['lsoa11', 'LSOA name', 'Composite Score']]

top_lsoas

"""The top three LSOAs with the best trade-off between the comprehensive health metrics (including health domain, green/bluespace, air quality, retail, and access to healthy assets and hazards) and crime rates are:

- LSOA Code: E01000916, LSOA Name: Camden 027B, Composite Score: 0.535
- LSOA Code: E01033489, LSOA Name: Islington 022G, Composite Score: 0.518
- LSOA Code: E01004316, LSOA Name: Tower Hamlets 009C, Composite Score: 0.512

These scores represent the balance of lower crime rates and better health assets, making these LSOAs potentially the most suitable for living based on the provided criteria and equal weighting of health factors.
"""

import folium
from folium.plugins import HeatMap

# Create a base map
m = folium.Map([health_crime_df['Latitude'].mean(), health_crime_df['Longitude'].mean()], zoom_start=7)

# Create a heat map layer for average crimes
heat_data = [[row['Latitude'], row['Longitude'], row['Average Crimes']] for index, row in health_crime_df.iterrows()]
HeatMap(heat_data, radius=10).add_to(m)

m  # Display the map

# Function to create a heatmap for a specific column with legend
def create_heatmap(data, lat, lon, column, map_title):
    # Create a base map
    m = folium.Map([data[lat].mean(), data[lon].mean()], zoom_start=7)

    # Create a heat map layer
    heat_data = [[row[lat], row[lon], row[column]] for index, row in data.iterrows()]
    HeatMap(heat_data, radius=10).add_to(m)

    # Adding title
    folium.map.Marker(
        [data[lat].max() + 0.01, data[lon].mean()],
        icon=folium.DivIcon(
            html=f"""<div style="font-family: courier new; color: black"><b>{map_title}</b></div>"""
            )
        ).add_to(m)

    return m

# Creating maps for each variable
maps = {}
variables = ['ah3h', 'ah3g', 'ah3e', 'ah3r', 'ah3ahah']
titles = ["Health Domain Score", "Green/Bluespace Domain Score", "Air Quality Domain Score", "Retail Domain Score", "Access to Healthy Assets and Hazards Index Score"]

for var, title in zip(variables, titles):
    maps[var] = create_heatmap(health_crime_df, 'Latitude', 'Longitude', var, title)

maps['ah3h']

maps['ah3g']

maps['ah3e']

maps['ah3r']

maps['ah3ahah']

"""## Analysis of Top Cities"""

# Extract city names from the LSOA names
health_crime_df['City'] = health_crime_df['LSOA name'].apply(lambda x: re.sub(r'\d+[A-Za-z]*$', '', x).strip())

# Prepare to aggregate and find top 10 cities for each variable
variables = {
    'ah3h': 'Health Domain Score',
    'ah3g': 'Green/Bluespace Domain Score',
    'ah3e': 'Air Quality Domain Score',
    'ah3r': 'Retail Domain Score',
    'ah3ahah': 'Access to Healthy Assets and Hazards Index'
}

# Dictionary to hold top 10 cities for each variable
top_10_cities = {}

# Calculate mean scores for each city and find top 10 cities for each variable
for var, desc in variables.items():
    # Group by city and calculate mean
    city_grouped = health_crime_df.groupby('City')[var].mean().reset_index()
    # Find top 10 cities with highest average scores
    top_10_cities[desc] = city_grouped.nlargest(10, var)

# Display the first top 10 list
top_10_cities['Health Domain Score']

# Function to create bar chart for top 10 cities
def create_bar_chart(top_cities, title, ylabel):
    plt.figure(figsize=(10, 6))
    plt.barh(top_cities['City'], top_cities.iloc[:, 1], color='skyblue')
    plt.xlabel('Average Score')
    plt.ylabel('City')
    plt.title(title)
    plt.gca().invert_yaxis()  # To display the highest score at the top
    plt.show()

# Create and display bar charts for each variable
for desc, top_cities in top_10_cities.items():
    create_bar_chart(top_cities, f'Top 10 Cities by {desc}', desc)

# Group by city and calculate mean for average crimes
city_crimes = health_crime_df.groupby('City')['Average Crimes'].mean().reset_index()

# Find top 10 cities with highest average crimes
top_10_crimes = city_crimes.nlargest(10, 'Average Crimes')

# Create bar chart for average crimes with x-axis labeled correctly

plt.figure(figsize=(10, 6))
plt.barh(top_10_crimes['City'], top_10_crimes['Average Crimes'], color='skyblue')
plt.xlabel('Average Crimes')
plt.ylabel('City')
plt.title('Top 10 Cities by Average Crimes')
plt.gca().invert_yaxis()  # To display the highest score at the top
plt.show()

top_10_cities

top_10_crimes

import geopandas as gpd
from shapely.geometry import Point
import pysal as ps
import libpysal
from esda.moran import Moran
from splot.esda import moran_scatterplot
import matplotlib.pyplot as plt
from libpysal.weights import KNN

# Creating a GeoDataFrame from the DataFrame
gdf = gpd.GeoDataFrame(health_crime_df, geometry=gpd.points_from_xy(health_crime_df.Longitude, health_crime_df.Latitude))

# Dropping rows with missing values in the 'Average Crimes' column or in the spatial data
gdf = gdf.dropna(subset=['Average Crimes', 'Latitude', 'Longitude'])

# Create a KNN weights matrix
w_knn = KNN.from_dataframe(gdf, k=5)  # You can adjust the number of neighbors (k)
w_knn.transform = 'R'

# Calculate Moran's I using the KNN weights
moran_knn = Moran(gdf['Average Crimes'], w_knn)
print("Moran's I (KNN): ", moran_knn.I, "P-value: ", moran_knn.p_sim)

import geopandas as gpd
import pandas as pd
from esda.getisord import G_Local
import libpysal

# Use KNN weights matrix
w_knn = KNN.from_dataframe(gdf, k=5)  # Adjust 'k' as needed
g_local_knn = G_Local(gdf['Average Crimes'], w_knn)

# Hotspot and Coldspot identification
hotspots = g_local_knn.Zs > 1.96  # Hotspots
coldspots = g_local_knn.Zs < -1.96  # Coldspots

# Add results to GeoDataFrame
gdf['g_local'] = g_local_knn.Zs
gdf['hotspot'] = hotspots
gdf['coldspot'] = coldspots

# Results
print(gdf[['g_local', 'hotspot', 'coldspot']])

