# -*- coding: utf-8 -*-
"""Data-Preparation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14JLgZ1HSBxcgp18BK5UglE0p0U_MYT77
"""

import os
import pandas as pd
import re

"""## Combine Yearly Crime Data"""

# Function to combine CSV files for each year
def combine_yearly_data(base_path, start_year=2020, end_year=2023):
    # Loop through each year
    for year in range(start_year, end_year + 1):
        yearly_data = []  # List to store data for the year
        # Loop through each month for the year
        for month in range(1, 13):
            month_str = f"{year}-{month:02d}"  # Format month correctly
            month_path = os.path.join(base_path, month_str)  # Construct the path to the month folder

            # Check if the month directory exists
            if os.path.exists(month_path):
                # List all csv files in the month directory
                for file_name in os.listdir(month_path):
                    if file_name.endswith('.csv'):
                        file_path = os.path.join(month_path, file_name)
                        df = pd.read_csv(file_path)  # Read the CSV file
                        yearly_data.append(df)  # Append the data to the yearly list
            else:
                print(f"Directory for {month_str} does not exist. Skipping.")

        # Combine all monthly data for the year
        combined_yearly_data = pd.concat(yearly_data, ignore_index=True)
        # Save the combined data to a CSV file
        combined_yearly_data.to_csv(os.path.join(base_path, f"{year}_combined.csv"), index=False)
        print(f"Data for the year {year} has been combined and saved.")

PATH = '2020-23-Crimes'
combine_yearly_data(PATH)

"""## Clean the CSV Files"""

def clean_csv_file(input_file_path, output_file_path):
    # Read the CSV file into a DataFrame
    df = pd.read_csv(input_file_path)

    # Print the status
    print(f"Cleaning file: {input_file_path}")

    # Drop the 'Crime ID' and 'Context' columns if they exist
    columns_to_drop = ['Crime ID', 'Context', 'Last outcome category']
    for col in columns_to_drop:
        if col in df.columns:
            df.drop(col, axis=1, inplace=True)

    # Remove duplicate records
    df_cleaned = df.drop_duplicates()

    # Remove records with missing values
    df_cleaned = df_cleaned.dropna()

    # Save the cleaned DataFrame to a new CSV file
    df_cleaned.to_csv(output_file_path, index=False)

    # Print the status
    print(f"Cleaned file saved to {output_file_path}")

# Paths for the input files and the output files
input_files = ['2020-23-Crimes/2020_combined.csv',
               '2020-23-Crimes/2021_combined.csv',
               '2020-23-Crimes/2022_combined.csv',
               '2020-23-Crimes/2023_combined.csv']

output_files = ['2020_cleaned.csv',
                '2021_cleaned.csv',
                '2022_cleaned.csv',
                '2023_cleaned.csv']

# Loop through the files and clean them
for in_file, out_file in zip(input_files, output_files):
    clean_csv_file(in_file, out_file)

"""## Combine all CSV Files"""

import pandas as pd

# File names for the CSV files in the current directory
file_names = [
    "2020_cleaned.csv",
    "2021_cleaned.csv",
    "2022_cleaned.csv",
    "2023_cleaned.csv"
]

# Read and combine all CSV files into a single DataFrame
combined_df = pd.concat([pd.read_csv(file_name) for file_name in file_names], ignore_index=True)

# Save the combined DataFrame to a new CSV file, if desired
combined_df.to_csv("combined_data.csv", index=False)

data = pd.read_csv('combined_data.csv')

# Rounding off Latitude and Longitude to 2 decimal places
data['Latitude'] = data['Latitude'].round(2)
data['Longitude'] = data['Longitude'].round(2)

# Extracting year from the 'Month' column
data['Year'] = pd.to_datetime(data['Month']).dt.year

# Grouping by 'Year', 'LSOA code', 'Latitude', 'Longitude', and 'LSOA name'
# and counting the number of crimes
grouped_data = data.groupby(['Year', 'LSOA code', 'Latitude', 'Longitude', 'LSOA name']).size().reset_index(name='Total Crimes')

# Saving the result to a CSV file
output_file_path = 'total_crime_count.csv'
grouped_data.to_csv(output_file_path, index=False)

print('File saved as', output_file_path)

# Extracting year from the 'Month' column
data['Year'] = pd.to_datetime(data['Month']).dt.year

# Grouping by 'Year', 'LSOA code', 'Latitude', 'Longitude', and 'LSOA name'
# and counting the number of crimes
grouped_data = data.groupby(['Year', 'LSOA code', 'LSOA name']).size().reset_index(name='Total Crimes')

# Saving the result to a CSV file
output_file_path = 'total_crime_count_LSOA.csv'
grouped_data.to_csv(output_file_path, index=False)

print('File saved as', output_file_path)

# Grouping the data by 'LSOA code' and calculating the average 'Latitude' and 'Longitude' for each LSOA code
# This will ensure that there's only one value for latitude and longitude for each LSOA code.

# Rounding off Latitude and Longitude to 2 decimal places
data['Latitude'] = data['Latitude'].round(2)
data['Longitude'] = data['Longitude'].round(2)

# Extracting year from the 'Month' column
data['Year'] = pd.to_datetime(data['Month']).dt.year

# Grouping the data by 'LSOA code' to calculate the average of 'Latitude' and 'Longitude'
lsoa_location_averages = data.groupby('LSOA code').agg({
    'Latitude': 'mean',
    'Longitude': 'mean'
}).reset_index()

# Rounding off the averaged latitude and longitude to 2 decimal places
lsoa_location_averages['Latitude'] = lsoa_location_averages['Latitude'].round(2)
lsoa_location_averages['Longitude'] = lsoa_location_averages['Longitude'].round(2)

# Merging the averaged locations back with the original data
data_with_avg_location = pd.merge(data, lsoa_location_averages, on='LSOA code', suffixes=('', '_avg'))

# Now we can group by 'Year', 'LSOA code', 'Latitude_avg', 'Longitude_avg', and 'LSOA name'
# to count the number of crimes
grouped_data_with_avg_location = data_with_avg_location.groupby(
    ['Year', 'LSOA code', 'Latitude_avg', 'Longitude_avg', 'LSOA name']
).size().reset_index(name='Total Crimes')

# Saving the result to a CSV file
output_file_path = 'total_crime_count_with_avg_location.csv'
grouped_data_with_avg_location.to_csv(output_file_path, index=False)

print('File saved as', output_file_path)

# let's calculate the average number of crimes for each LSOA code
# across the years 2020, 2021, 2022, and 2023 and remove the 'Year' column.
crime_data = pd.read_csv('total_crime_count_with_avg_location.csv')

# Grouping by 'LSOA code', 'Avg Latitude', 'Avg Longitude', and 'LSOA name'
# to calculate the average number of crimes over the specified years
crime_data['Avg Latitude'] = crime_data['Avg Latitude'].round(2)
crime_data['Avg Longitude'] = crime_data['Avg Longitude'].round(2)

average_crimes = crime_data.groupby(['LSOA code', 'Avg Latitude', 'Avg Longitude', 'LSOA name'])['Total Crimes'].mean().reset_index()

# Renaming the column to reflect that it's the average
average_crimes = average_crimes.rename(columns={'Total Crimes': 'Average Crimes'})

# Saving the result to a CSV file without the 'Year' column
output_file_path_avg_crimes = 'average_crimes_2020_to_2023.csv'
average_crimes.to_csv(output_file_path_avg_crimes, index=False)

print('File saved as', output_file_path_avg_crimes)

# Load the datasets
ahah_dataset_path = 'AHAH_V3_0.csv'  # Replace with the actual file path
total_crime_dataset_path = 'total_crime_count_with_avg_location.csv'  # Replace with the actual file path

ahah_df = pd.read_csv(ahah_dataset_path)
total_crime_df = pd.read_csv(total_crime_dataset_path)

# Function to remove apostrophes from all string columns in a dataframe
def remove_apostrophes(df):
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = df[col].str.replace("'", "")
    return df

# Clean both datasets
ahah_df_cleaned = remove_apostrophes(ahah_df.copy())

# Display the first few rows of each cleaned dataset for verification
ahah_df_cleaned.head()

# Columns to clean
columns_to_clean = ['ah3no2', 'ah3so2', 'ah3pm10', 'ah3g', 'ah3e', 'ah3r', 'ah3ahah']

# Remove apostrophes from specific columns
for col in columns_to_clean:
    if col in ahah_df_cleaned.columns:
        ahah_df_cleaned[col] = ahah_df_cleaned[col].astype(str).str.replace("'", "")

# Display the first few rows of the cleaned columns for verification
ahah_df_cleaned[columns_to_clean].head()

# Grouping by LSOA code and Year to calculate the sum of crimes for each year
grouped_crime_data = total_crime_df.groupby(['LSOA code', 'Year'])['Total Crimes'].sum().unstack()

# Calculating the average number of crimes over the years 2020-2023
grouped_crime_data['Average Crimes'] = grouped_crime_data.mean(axis=1)

# Resetting index to make 'LSOA code' a column again
grouped_crime_data.reset_index(inplace=True)

# Creating a reduced version of the total crime dataset with unique LSOA code and corresponding LSOA name
unique_lsoa_names_df = total_crime_df[['LSOA code', 'LSOA name']].drop_duplicates()

# Merging the unique LSOA names with the grouped crime data
grouped_crime_data_with_names = pd.merge(grouped_crime_data, unique_lsoa_names_df, on='LSOA code')

# Merging the grouped crime data (with names) back with the AHAH dataset
final_merged_df = pd.merge(ahah_df_cleaned, grouped_crime_data_with_names, left_on='lsoa11', right_on='LSOA code')

# Selecting columns for the final dataset
columns_to_keep = list(ahah_df_cleaned.columns) + ['LSOA name', 2020, 2021, 2022, 2023, 'Average Crimes']
final_dataset = final_merged_df[columns_to_keep]

# Save the final dataset to CSV
final_csv_path = 'combined_health_crime_data.csv'  # Replace with the desired file path
final_dataset.to_csv(final_csv_path, index=False)

print("The CSV file is saved as 'combined_health_crime_data.csv'")

# Load the datasets
ahah_data = pd.read_csv('AHAH_V3_0.csv')
crime_data = pd.read_csv('total_crime_count_with_avg_location.csv')

# Extract the LSOA codes from both datasets
ahah_lsoa_codes = set(ahah_data['lsoa11'])
crime_lsoa_codes = set(crime_data['LSOA code'])

# Calculate the intersections and differences
matching_lsoa_codes = ahah_lsoa_codes.intersection(crime_lsoa_codes)
lsoa_in_ahah_not_in_crime = ahah_lsoa_codes.difference(crime_lsoa_codes)
lsoa_in_crime_not_in_ahah = crime_lsoa_codes.difference(ahah_lsoa_codes)

# Count the numbers for each
count_matching = len(matching_lsoa_codes)
count_ahah_not_crime = len(lsoa_in_ahah_not_in_crime)
count_crime_not_ahah = len(lsoa_in_crime_not_in_ahah)

print('There are '+str(count_matching)+' LSOA codes in the crime data that match with the lsoa11 codes in the AHAH data.')

print('LSOA codes common in AHAH data and crime data: '+str(count_matching))
print('LSOA codes in AHAH data but not in crime data: '+str(count_ahah_not_crime))
print('LSOA codes in crime data but not in AHAH data: '+str(count_crime_not_ahah))

"""Getting LSOA codes for the combined dataset"""

# Load the primary dataset
primary_data = pd.read_csv('combined_health_crime_data.csv')

# Display the first few rows of the primary dataset
primary_data.head()

# Load the average crime dataset
average_crime_data = pd.read_csv('average_crimes_2020_to_2023.csv')

# Display the first few rows of the average crime dataset
average_crime_data.head()

# Rename the columns in the average crime dataset for consistency
average_crime_data.rename(columns={'LSOA code': 'lsoa11', 'Avg Latitude': 'Latitude', 'Avg Longitude': 'Longitude'}, inplace=True)

# Merge the datasets on the 'lsoa11' column
merged_data = pd.merge(primary_data, average_crime_data[['lsoa11', 'Latitude', 'Longitude']], on='lsoa11', how='left')

# Check the first few rows of the merged dataset
merged_data.head()

# Save the merged dataset to a new CSV file
updated_dataset_path = 'AHAH_Crimes.csv'
merged_data.to_csv(updated_dataset_path, index=False)

print('Updated dataset saved as:', updated_dataset_path)

